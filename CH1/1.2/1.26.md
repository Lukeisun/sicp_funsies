Despite looking at some other answers I found my self more confused as I didn't 
understand their answers or thought they were too handwavey.

But I believe the idea is we are now calling 2 recursive functions, adding another
recursive call increases the time complexity exponentially. I do not remember where I read
this but it is either from this book or Skiena's "Algorithm Design Manual" as im reading both concurrently
So from O(logn) we go to
`O(log(2^n)) ~> O(nlog(2)) ~> O(n)`

Trying to explain this another way,
in this example, if we call `expmod` the normal way.
Our tree looks like

        o  
       /
      o
     /
    ...

Which is from my understanding "linearly recursive"

But making 2 calls to `expmod` our tree looks like this

        o  
       /  \
      o     o
     / \   / \
    o   o o   o

Now we are doing tree recursion.
And our number of nodes has increased exponentially.
The number nodes of a full binary tree can be calculated with `2^h -1` where h is the height of the tree
Given the function we see that `exp` is directly related to the height the tree so we can substitute exp
in our formula. `2^exp - 1`. So now we have `log(2^exp - 1)`, dropping off the 1 as its negligble in 
asymptotic analysis `log(2^exp)` and its clear it is `O(n)` growth.

Apologies if you're reading this and it's confusing.
I think my mental model here is correct but please let me know if I have made a mistake.
